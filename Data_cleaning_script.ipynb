{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Checker\n",
    "a program that we can run the data through to check stuff - just need to update the \"data\" line to use the most recent csv\n",
    "\n",
    "Things to check:\n",
    "- all latitudes and longitudes are legit values\n",
    "- that there are no repeats of photoID\n",
    "- that there are no repeats of URL\n",
    "- Check that URLs are all legit (200ok response or smth)\n",
    "- that AlbumID and AlbumTitle match\n",
    "- lat and long match the county\n",
    "- lat and long match the state\n",
    "- county matches state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\csky2\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\geopandas\\_compat.py:123: UserWarning: The Shapely GEOS version (3.10.3-CAPI-1.16.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import validators\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing flickr data\n",
    "data = pd.read_csv('tidied_threaded_data_pull.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Latitude and longitude are legit values.\n",
    "Latitude should be between -90, 90.\n",
    "Longitude should be between -180, 180."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of invalid latitudes: 0.0\n"
     ]
    }
   ],
   "source": [
    "lat_data = data['Latitude'].dropna()\n",
    "print('Number of invalid latitudes:', lat_data[lat_data.between(-90, 90) == False].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of invalid longitudes: 0.0\n"
     ]
    }
   ],
   "source": [
    "lon_data = data['Longitude'].dropna()\n",
    "print('Number of invalid longitudes:', lon_data[lon_data.between(-180, 80) == False].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. There are no repeats of PhotoID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of repeat PhotoIDs: 0\n"
     ]
    }
   ],
   "source": [
    "print('Number of repeat PhotoIDs:', data['PhotoID'].value_counts()[data['PhotoID'].value_counts() > 1].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. There are no repeats of URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of repeat URLs: 0\n"
     ]
    }
   ],
   "source": [
    "print('Number of repeat URLs:', data['URL'].value_counts()[data['URL'].value_counts() > 1].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. URL calls are valid.\n",
    "Faster to use the validators library than loop to check 200 responses (https://validators.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of invalid URLs: 0\n"
     ]
    }
   ],
   "source": [
    "def is_valid_url(url):\n",
    "    return validators.url(url)\n",
    "\n",
    "url_checker_df = pd.DataFrame({'URL': data['URL'], 'check': data['URL'].apply(is_valid_url)})\n",
    "\n",
    "print('Number of invalid URLs:', url_checker_df[url_checker_df['check'] == False]['check'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. AlbumIDs match AlbumTitles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to update the scrape to include albumID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The latitude and longitude point match the county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading county and state shapefiles to check location matches\n",
    "# https://www.census.gov/cgi-bin/geo/shapefiles/index.php\n",
    "counties_gdf = gpd.read_file('tl_2023_us_county/tl_2023_us_county.shp') \n",
    "states_gdf = gpd.read_file('tl_2023_us_state/tl_2023_us_state.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use to test long/lat county/state checker - ONLY FOR TESTING CODE, DELETE WHEN RUNNING CHECKER ON FINAL DATASET\n",
    "data = pd.read_csv('attribute_table.csv')\n",
    "data.drop_duplicates(subset=['Latitude', 'Longitude', 'County', 'State'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining helper functions\n",
    "def process_area(area):\n",
    "    if area not in [\"County\", \"State\"]:\n",
    "         raise ValueError(\"Parameter 'area' must be 'County' or 'State'\")\n",
    "        \n",
    "    # Function logic here\n",
    "    if area == \"County\":\n",
    "        gdf_name = 'NAME'\n",
    "        print(\"Processing County Matches\")\n",
    "    elif area == \"State\":\n",
    "        gdf_name = 'STUSPS'\n",
    "        print(\"Processing State Matches\")\n",
    "    return gdf_name\n",
    "\n",
    "def fix_text(phrase):\n",
    "    name = re.sub(r'county', '', phrase, flags=re.IGNORECASE).strip()\n",
    "    name = re.sub(r'[^a-zA-Z]', '', name)\n",
    "    return name.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_match(flickr_data, gpd_df, area):\n",
    "\n",
    "    gdf_name = process_area(area)\n",
    "\n",
    "    locations = flickr_data[['Latitude', 'Longitude', area]]\n",
    "\n",
    "    geometry = [Point(xy) for xy in zip(locations['Longitude'], locations['Latitude'])]\n",
    "    geo_data = gpd.GeoDataFrame(locations, geometry=geometry)\n",
    "    geo_data = geo_data.set_crs(gpd_df.crs)\n",
    "\n",
    "    county_latlon_join = gpd.sjoin(geo_data, gpd_df, how='left', op='within')[[area, gdf_name]]\n",
    "\n",
    "    county_latlon_join[area] = county_latlon_join[area].astype('str').apply(fix_text)\n",
    "    county_latlon_join[gdf_name] = county_latlon_join[gdf_name].astype('str').apply(fix_text)\n",
    "\n",
    "    county_latlon_join['match'] = county_latlon_join[area].str.lower() == county_latlon_join[gdf_name].str.lower()\n",
    "\n",
    "    nan_mismatches = county_latlon_join[(county_latlon_join['match'] == False) & ((county_latlon_join[area] == 'nan') | (county_latlon_join[gdf_name] == 'nan'))]\n",
    "\n",
    "    name_mismatches = county_latlon_join[county_latlon_join['match'] == False].drop(index = nan_mismatches.index)\n",
    "\n",
    "    return nan_mismatches, name_mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing County Matches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\csky2\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:3398: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "county_nan_mismatches, county_name_mismatches = location_match(data, counties_gdf, 'County')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of county-coordinate mismatches due to missing data: 1299\n",
      "Number of county-coordinate name mismatches: 93\n",
      "Percent of data where counties are mismatched: 68.70681145113524\n"
     ]
    }
   ],
   "source": [
    "print('Number of county-coordinate mismatches due to missing data:', county_nan_mismatches.shape[0])\n",
    "print('Number of county-coordinate name mismatches:', county_name_mismatches.shape[0])\n",
    "print('Percent of data where counties are mismatched:', (county_nan_mismatches.shape[0] + county_name_mismatches.shape[0])/data.shape[0]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. The latitude and longitude point match the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing State Matches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\csky2\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:3398: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "state_nan_mismatches, state_name_mismatches =location_match(data, states_gdf, 'State')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of state-coordinate mismatches due to missing data: 45\n",
      "Number of state-coordinate name mismatches: 83\n",
      "Percent of data where states are mismatched: 6.317867719644619\n"
     ]
    }
   ],
   "source": [
    "print('Number of state-coordinate mismatches due to missing data:', state_nan_mismatches.shape[0])\n",
    "print('Number of state-coordinate name mismatches:', state_name_mismatches.shape[0])\n",
    "print('Percent of data where states are mismatched:', (state_nan_mismatches.shape[0] + state_name_mismatches.shape[0])/data.shape[0]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. The county and state match each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_gdf['STATEFP'] = counties_gdf['STATEFP'].astype('int')\n",
    "states_gdf['STATEFP'] = states_gdf['STATEFP'].astype('int')\n",
    "\n",
    "def fix_text2(phrase):\n",
    "    if type(phrase) != int:\n",
    "        name = re.sub(r'county', '', str(phrase), flags=re.IGNORECASE).strip()\n",
    "        name = re.sub(r'[^a-zA-Z]', '', name)\n",
    "        rtn = name.strip()\n",
    "    else:\n",
    "        rtn = phrase\n",
    "    return rtn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties = counties_gdf[['NAME', 'STATEFP']].apply(lambda x: x.apply(fix_text2)).rename(columns= {'NAME': 'County', 'STATEFP': 'StateID'})\n",
    "states = states_gdf[['STUSPS', 'STATEFP']].apply(lambda x: x.apply(fix_text2)).rename(columns= {'STUSPS': 'State', 'STATEFP': 'StateID'})\n",
    "area_data = data[['County', 'State']].apply(lambda x: x.apply(fix_text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(counties, states, on='StateID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_index_area = pd.MultiIndex.from_frame(area_data[['County', 'State']])\n",
    "multi_index_merged = pd.MultiIndex.from_frame(merged_df[['County', 'State']])\n",
    "area_data['match'] = area_data.set_index(['County', 'State']).index.isin(multi_index_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of photos with mismatched county and state: 58\n",
      "Percent of photos with mismatched county and state: 2.8627838104639687\n"
     ]
    }
   ],
   "source": [
    "mismatch_county_state = area_data[(area_data['match'] == False) & (area_data['County'] != 'nan') & (area_data['State'] != 'nan')]\n",
    "print('Number of photos with mismatched county and state:', mismatch_county_state.shape[0])\n",
    "print('Percent of photos with mismatched county and state:', mismatch_county_state.shape[0]/data.shape[0]*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
