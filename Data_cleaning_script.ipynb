{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Checker\n",
    "a program that we can run the data through to check data validity - just need to update the \"data\" line to use the most recent csv\n",
    "\n",
    "Things to check:\n",
    "- all latitudes and longitudes are legit values\n",
    "- that there are no repeats of photoID\n",
    "- that there are no repeats of URL\n",
    "- Check that URLs are all legit (200 response)\n",
    "- that AlbumID and AlbumTitle match\n",
    "- lat and long match the county\n",
    "- lat and long match the state\n",
    "- county matches state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import validators\n",
    "import re\n",
    "import concurrent.futures\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing flickr data\n",
    "data = pd.read_csv('tidied_threaded_data_pull.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Latitude and longitude are legit values.\n",
    "Latitude should be between -90, 90.\n",
    "Longitude should be between -180, 180."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of invalid latitudes: 0.0\n"
     ]
    }
   ],
   "source": [
    "lat_data = data['Latitude'].dropna()\n",
    "print('Number of invalid latitudes:', lat_data[lat_data.between(-90, 90) == False].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of invalid longitudes: 0.0\n"
     ]
    }
   ],
   "source": [
    "lon_data = data['Longitude'].dropna()\n",
    "print('Number of invalid longitudes:', lon_data[lon_data.between(-180, 180) == False].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. There are no repeats of PhotoID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of repeat PhotoIDs: 0\n"
     ]
    }
   ],
   "source": [
    "print('Number of repeat PhotoIDs:', data['PhotoID'].value_counts()[data['PhotoID'].value_counts() > 1].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. There are no repeats of URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of repeat URLs: 0\n"
     ]
    }
   ],
   "source": [
    "print('Number of repeat URLs:', data['URL'].value_counts()[data['URL'].value_counts() > 1].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. URL calls are valid and good status.\n",
    "Faster to use the validators library (https://validators.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of invalid URLs: 0\n"
     ]
    }
   ],
   "source": [
    "def is_valid_url(url):\n",
    "    return validators.url(url)\n",
    "\n",
    "url_checker_df = pd.DataFrame({'URL': data['URL'], 'check': data['URL'].apply(is_valid_url)})\n",
    "\n",
    "print('Number of invalid URLs:', url_checker_df[url_checker_df['check'] == False]['check'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the URLs to have a 200 response. If they return any response other than 200, we will check again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 124.63 s\n",
      "Number of URLs with bad status: 0\n"
     ]
    }
   ],
   "source": [
    "urls = data['URL'].to_list()\n",
    "\n",
    "def check_url_status(urls):\n",
    "    CONNECTIONS = 10\n",
    "    TIMEOUT = 15\n",
    "\n",
    "    out = []\n",
    "\n",
    "    # Function to load URL and get status code\n",
    "    def load_url(url, timeout):\n",
    "        try:\n",
    "            ans = requests.head(url, timeout=timeout)\n",
    "            return ans.status_code\n",
    "        except Exception as exc:\n",
    "            return str(type(exc))\n",
    "\n",
    "    # Use ThreadPoolExecutor for concurrent requests\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=CONNECTIONS) as executor:\n",
    "        future_to_url = {executor.submit(load_url, url, TIMEOUT): url for url in urls}\n",
    "        time1 = time.time()\n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                status_code = future.result()\n",
    "            except Exception as exc:\n",
    "                status_code = str(type(exc))\n",
    "            finally:\n",
    "                out.append((url, status_code))\n",
    "                print(str(len(out)), end=\"\\r\")\n",
    "\n",
    "        time2 = time.time()\n",
    "\n",
    "    print(f'Took {time2-time1:.2f} s')\n",
    "\n",
    "    # Ensure the length of URLs and statuses match\n",
    "    assert len(urls) == len(out), \"The lengths of URLs and their status codes do not match.\"\n",
    "\n",
    "    url_status_df = pd.DataFrame(out, columns=['URL', 'status'])\n",
    "    return url_status_df\n",
    "\n",
    "# Initial URL check\n",
    "url_status_df = check_url_status(urls)\n",
    "\n",
    "print('Number of URLs with bad status:', url_status_df[url_status_df['status'] != 200].shape[0])\n",
    "\n",
    "# Accumulate all results\n",
    "all_results = url_status_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop to recheck URLs that did not return status 200 - failsafe to double check, sometimes some of the URLs come back w bad status because of the speed of checking them/overloading flickr\n",
    "# but this is faster than individually looping through\n",
    "while url_status_df[url_status_df['status'] != 200].shape[0] > 0:\n",
    "    urls = url_status_df[url_status_df['status'] != 200]['URL'].to_list()\n",
    "    url_status_df = check_url_status(urls)\n",
    "    print('Number of URLs with bad status:', url_status_df[url_status_df['status'] != 200].shape[0])\n",
    "\n",
    "    all_results = pd.concat([all_results, url_status_df]).drop_duplicates(subset='URL', keep='last')\n",
    "    \n",
    "final_results = all_results.drop_duplicates(subset='URL', keep='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. AlbumIDs match AlbumTitles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to update the scrape to include albumID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The latitude and longitude point match the county.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading county and state shapefiles to check location matches\n",
    "# https://www.census.gov/cgi-bin/geo/shapefiles/index.php\n",
    "counties_gdf = gpd.read_file('tl_2023_us_county/tl_2023_us_county.shp') \n",
    "states_gdf = gpd.read_file('tl_2023_us_state/tl_2023_us_state.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use to test long/lat county/state checker - ONLY FOR TESTING CODE, DELETE WHEN RUNNING CHECKER ON FINAL DATASET\n",
    "data = pd.read_csv('attribute_table.csv')\n",
    "data.drop_duplicates(subset=['Latitude', 'Longitude', 'County', 'State'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining helper functions\n",
    "def process_area(area):\n",
    "    if area not in [\"County\", \"State\"]:\n",
    "         raise ValueError(\"Parameter 'area' must be 'County' or 'State'\")\n",
    "        \n",
    "    # Function logic here\n",
    "    if area == \"County\":\n",
    "        gdf_name = 'NAME'\n",
    "        print(\"Processing County Matches\")\n",
    "    elif area == \"State\":\n",
    "        gdf_name = 'STUSPS'\n",
    "        print(\"Processing State Matches\")\n",
    "    return gdf_name\n",
    "\n",
    "def fix_text(phrase):\n",
    "    name = re.sub(r'county', '', phrase, flags=re.IGNORECASE).strip()\n",
    "    name = re.sub(r'[^a-zA-Z]', '', name)\n",
    "    return name.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_match(flickr_data, gpd_df, area):\n",
    "\n",
    "    gdf_name = process_area(area)\n",
    "\n",
    "    locations = flickr_data[['Latitude', 'Longitude', area]]\n",
    "\n",
    "    geometry = [Point(xy) for xy in zip(locations['Longitude'], locations['Latitude'])]\n",
    "    geo_data = gpd.GeoDataFrame(locations, geometry=geometry)\n",
    "    geo_data = geo_data.set_crs(gpd_df.crs)\n",
    "\n",
    "    county_latlon_join = gpd.sjoin(geo_data, gpd_df, how='left', op='within')[[area, gdf_name]]\n",
    "\n",
    "    county_latlon_join[area] = county_latlon_join[area].astype('str').apply(fix_text)\n",
    "    county_latlon_join[gdf_name] = county_latlon_join[gdf_name].astype('str').apply(fix_text)\n",
    "\n",
    "    county_latlon_join['match'] = county_latlon_join[area].str.lower() == county_latlon_join[gdf_name].str.lower()\n",
    "\n",
    "    nan_mismatches = county_latlon_join[(county_latlon_join['match'] == False) & ((county_latlon_join[area] == 'nan') | (county_latlon_join[gdf_name] == 'nan'))]\n",
    "\n",
    "    name_mismatches = county_latlon_join[county_latlon_join['match'] == False].drop(index = nan_mismatches.index)\n",
    "\n",
    "    return nan_mismatches, name_mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing County Matches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\csky2\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:3398: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "county_nan_mismatches, county_name_mismatches = location_match(data, counties_gdf, 'County')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of county-coordinate mismatches due to missing data: 1299\n",
      "Number of county-coordinate name mismatches: 93\n",
      "Percent of data where counties are mismatched: 68.70681145113524\n"
     ]
    }
   ],
   "source": [
    "print('Number of county-coordinate mismatches due to missing data:', county_nan_mismatches.shape[0])\n",
    "print('Number of county-coordinate name mismatches:', county_name_mismatches.shape[0])\n",
    "print('Percent of data where counties are mismatched:', (county_nan_mismatches.shape[0] + county_name_mismatches.shape[0])/data.shape[0]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. The latitude and longitude point match the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing State Matches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\csky2\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:3398: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "state_nan_mismatches, state_name_mismatches =location_match(data, states_gdf, 'State')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of state-coordinate mismatches due to missing data: 45\n",
      "Number of state-coordinate name mismatches: 83\n",
      "Percent of data where states are mismatched: 6.317867719644619\n"
     ]
    }
   ],
   "source": [
    "print('Number of state-coordinate mismatches due to missing data:', state_nan_mismatches.shape[0])\n",
    "print('Number of state-coordinate name mismatches:', state_name_mismatches.shape[0])\n",
    "print('Percent of data where states are mismatched:', (state_nan_mismatches.shape[0] + state_name_mismatches.shape[0])/data.shape[0]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. The county and state match each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_gdf['STATEFP'] = counties_gdf['STATEFP'].astype('int')\n",
    "states_gdf['STATEFP'] = states_gdf['STATEFP'].astype('int')\n",
    "\n",
    "def fix_text2(phrase):\n",
    "    if type(phrase) != int:\n",
    "        name = re.sub(r'county', '', str(phrase), flags=re.IGNORECASE).strip()\n",
    "        name = re.sub(r'[^a-zA-Z]', '', name)\n",
    "        rtn = name.strip()\n",
    "    else:\n",
    "        rtn = phrase\n",
    "    return rtn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties = counties_gdf[['NAME', 'STATEFP']].apply(lambda x: x.apply(fix_text2)).rename(columns= {'NAME': 'County', 'STATEFP': 'StateID'})\n",
    "states = states_gdf[['STUSPS', 'STATEFP']].apply(lambda x: x.apply(fix_text2)).rename(columns= {'STUSPS': 'State', 'STATEFP': 'StateID'})\n",
    "area_data = data[['County', 'State']].apply(lambda x: x.apply(fix_text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(counties, states, on='StateID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_index_area = pd.MultiIndex.from_frame(area_data[['County', 'State']])\n",
    "multi_index_merged = pd.MultiIndex.from_frame(merged_df[['County', 'State']])\n",
    "area_data['match'] = area_data.set_index(['County', 'State']).index.isin(multi_index_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of photos with mismatched county and state: 58\n",
      "Percent of photos with mismatched county and state: 2.8627838104639687\n"
     ]
    }
   ],
   "source": [
    "mismatch_county_state = area_data[(area_data['match'] == False) & (area_data['County'] != 'nan') & (area_data['State'] != 'nan')]\n",
    "print('Number of photos with mismatched county and state:', mismatch_county_state.shape[0])\n",
    "print('Percent of photos with mismatched county and state:', mismatch_county_state.shape[0]/data.shape[0]*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
